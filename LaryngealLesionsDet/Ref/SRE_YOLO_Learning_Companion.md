# SRE-YOLO ML Learning Companion

**Key Concepts, Explanations & Study Resources for Every Section**

> Baldini et al. (2025) Â· Comput. Methods Programs Biomed. 260:108539

---

## ðŸ“– How to Use This Document

- Before starting each section in the implementation spec, read the matching section here first.
- Watch the linked YouTube videos for any concept you are not yet confident with.
- Concepts are rated ðŸŸ¢ Beginner Â· ðŸŸ¡ Intermediate Â· ðŸ”µ Advanced.
- You do not need to master every concept before starting â€” learn as you build!

---

## ðŸ› ï¸ Section 1 â€” Environment Setup

> Before you can write a single line of ML code, you need a properly configured Python environment. These concepts underpin every step that follows.

---

### Conda & Virtual Environments Â· ðŸŸ¢ Beginner

A virtual environment is an isolated Python installation that keeps your project's dependencies separate from other projects. Conda is a popular environment manager that handles both Python versions and packages. Without it, installing one project's libraries can break another's.

**ðŸ”— In this project:** You'll create a `sre_yolo` environment with Python 3.10 and every library pinned to an exact version, ensuring your training runs are reproducible.

- â–¶ [Conda Tutorial for Beginners â€” Anaconda](https://www.youtube.com/watch?v=sDCtY68QM4s) â€” 15 min intro to conda environments
- â–¶ [Python Virtual Environments Full Guide](https://www.youtube.com/watch?v=IAvAlS0CuxI) â€” covers venv, pip, and conda differences

---

### pip & Package Management Â· ðŸŸ¢ Beginner

pip is Python's package installer. It downloads libraries from PyPI (the Python Package Index). "Pinning" a version (e.g. `numpy==1.26.4`) means everyone gets the exact same code, preventing the classic "works on my machine" bug.

**ðŸ”— In this project:** `environment.yml` pins every library (PyTorch, Ultralytics, OpenCV, etc.) so the training results from this paper can be exactly reproduced.

- â–¶ [pip Tutorial â€” How to Install Python Packages](https://www.youtube.com/watch?v=U2ZN104hIcc) â€” pip basics in 10 min
- â–¶ [Python Packaging Explained](https://www.youtube.com/watch?v=YM6cz5OHf14) â€” requirements.txt vs environment.yml

---

### CUDA & GPU Computing Â· ðŸŸ¡ Intermediate

CUDA is NVIDIA's platform for running code on GPUs. Neural network training is fundamentally matrix multiplication at scale â€” GPUs do this 10â€“100Ã— faster than CPUs because they have thousands of small parallel cores. PyTorch uses CUDA to accelerate all tensor operations.

**ðŸ”— In this project:** YOLOv8n achieves 58.8 FPS because inference runs on a GPU. Without CUDA, training would take weeks instead of days.

- â–¶ [But what is a GPU? (3Blue1Brown style)](https://www.youtube.com/watch?v=r9IQDQkSv_w) â€” visual intuition for GPU parallelism
- â–¶ [CUDA Programming â€” Why GPUs for Deep Learning](https://www.youtube.com/watch?v=EMtPv1_bVMI) â€” explains why matrix ops map to GPU cores
- â–¶ [PyTorch GPU Setup Guide](https://www.youtube.com/watch?v=UWlFM0R_x6I) â€” practical CUDA setup for deep learning

---

### PyTorch Fundamentals Â· ðŸŸ¢ Beginner

PyTorch is the deep learning framework used throughout this project. It provides tensors (n-dimensional arrays like NumPy, but GPU-accelerated), automatic differentiation (autograd), and pre-built building blocks for neural networks (`nn.Module`).

**ðŸ”— In this project:** Every model, loss function, and training loop is written in PyTorch. Understanding tensors and `nn.Module` is the single most important prerequisite.

- â–¶ [PyTorch for Deep Learning â€” Full Course (freeCodeCamp)](https://www.youtube.com/watch?v=V_xro1bcAuA) â€” comprehensive 25-hour course â€” watch Â§1â€“3 first
- â–¶ [PyTorch in 100 Seconds](https://www.youtube.com/watch?v=ORMx45xqWkA) â€” quick overview of tensors and autograd
- â–¶ [nn.Module Explained](https://www.youtube.com/watch?v=GIkg3DkESA4) â€” how to build neural network layers in PyTorch

---

## ðŸ“ Section 2 â€” Project Scaffold & Configuration

> Good project structure is not just tidiness â€” it makes the difference between code you can debug in 6 months and code that is a maze.

---

### ML Project Structure Â· ðŸŸ¢ Beginner

A well-organised ML project separates concerns: data loading code lives in one place, model definitions in another, training logic in another. This mirrors software engineering best practices and makes it easy to swap out components.

**ðŸ”— In this project:** The `sre_yolo/` scaffold has separate folders for `models/`, `data_utils/`, `training/`, and `evaluation/` â€” each section of the spec maps to exactly one folder.

- â–¶ [Structuring Machine Learning Projects](https://www.youtube.com/watch?v=MUqNwgPjJvQ) â€” best practices from industry ML engineers
- â–¶ [Cookiecutter Data Science â€” Project Templates](https://www.youtube.com/watch?v=2VuKIxzAyTE) â€” standard ML folder conventions explained

---
## ðŸ“‚ Project File & Folder Reference â€” What Each File Does

> Think of the project as an assembly line. Data flows left to right through each component:

```
Raw Images  â†’  data_utils/  â†’  models/  â†’  training/  â†’  evaluation/  â†’  results/
                                   â†‘
                             configs/  (controls everything)
```

---

### ðŸ“ `configs/` â€” The Control Panel

No code here â€” only settings. Every other file reads from this folder. Changing a hyperparameter means editing one line here, not hunting through ten Python files.

| File | What it does | Who reads it | Output |
|---|---|---|---|
| `configs/train.yaml` | All 19 hyperparameters: lr, batch size, loss weights, image sizes, SR layer indices | trainer.py, loss.py, dataset.py â€” everything | None (read-only) |
| `configs/dataset.yaml` | Paths to data folders, class count (`nc: 1`), class names (`['lesion']`) | dataset.py, preprocess.py | None (read-only) |
| `configs/model.yaml` | Architecture flags â€” which backbone variant to use | sre_yolo.py | None (read-only) |

**Key values in `train.yaml` and why they matter:**

| Parameter | Value | Controls |
|---|---|---|
| `lr0` | 0.05 | Initial learning rate â€” how big each weight update step is |
| `epochs` | 100 | How many full passes through all training data |
| `batch_size` | 16 | How many images processed simultaneously on GPU |
| `c1` | 0.1 | SR loss weight â€” how much super-resolution influences training |
| `c2` | 7.5 | Bounding box loss weight â€” the primary detection objective |
| `c3` | 1.5 | DFL loss weight â€” box boundary distribution accuracy |
| `c4` | 0.5 | Classification loss weight â€” lesion vs background |
| `sr_layers` | [4, 8] | Which backbone layers feed the SR branch |
| `imgsz_lr` | 640 | Input image size to the detector |
| `imgsz_hr` | 1280 | Target image size for super-resolution reconstruction |

---

### ðŸ“ `data/` â€” All Images and Split Lists

Never contains Python code â€” only image files, label files, and text files listing which images belong to each split.

**`data/raw/`**
Your original downloaded images, exactly as received. Never modified. `preprocess.py` reads from here.
- Output: Nothing new â€” just a safe backup of originals.

**`data/processed/lr/`**
All images resized to **640Ã—640** â€” the exact size YOLOv8n expects. These are the actual images fed to the model every training step.
```
data/processed/lr/
    images/   â† 640Ã—640 JPEG images
    labels/   â† YOLO .txt files (one per image, listing bounding boxes)
```

**`data/processed/hr/`**
Same images at **1280Ã—1280** â€” the super-resolution reconstruction target. The SR branch tries to recreate these from the 640Ã—640 input. The L1 loss compares SR branch output against these.
```
data/processed/hr/
    images/   â† 1280Ã—1280 JPEG images
    # NO labels â€” HR is only used as an SR target, not for detection
```

**`data/splits/split1/` `split2/` `split3/`**
Each folder contains three plain text files. Every line is one absolute path to an LR image.
```
data/splits/split1/
    train.txt   â† ~3,100 lines
    val.txt     â† ~240 lines
    test.txt    â† ~135 lines
```
`dataset.py` reads these to know which images to load for each training/validation/test phase.

---

### ðŸ“ `data_utils/` â€” Everything That Touches Data Before the Model

**`data_utils/preprocess.py`**
Three functions that run **once** before training to set up all data:

1. `filter_frames()` â€” scans `data/raw/`, removes blurry/dark/tiny images using Laplacian variance
2. `create_dual_resolution()` â€” produces both 640Ã—640 LR and 1280Ã—1280 HR versions of each image
3. `make_splits()` â€” groups images by patient, shuffles, writes train/val/test `.txt` files

Output:
```
results/filter_log.csv          â† log of every excluded image + reason
data/processed/lr/images/       â† LR images
data/processed/lr/labels/       â† YOLO label files
data/processed/hr/images/       â† HR images
data/splits/split{1,2,3}/*.txt  â† split file lists
```

**`data_utils/dataset.py`**
Defines `SREYOLODataset` â€” the class PyTorch calls thousands of times per epoch to load one sample. `__getitem__()` reads one image path from `train.txt`, loads the LR image, loads the matching HR image, loads bounding box labels, and returns them as GPU-ready tensors.

Output per `__getitem__()` call:
```python
{
  "lr_image": FloatTensor [3, 640, 640],    # fed to YOLOv8 backbone
  "hr_image": FloatTensor [3, 1280, 1280],  # compared against SR output
  "labels":   FloatTensor [N, 5],           # (class_id, cx, cy, w, h)
  "img_path": str                           # for debugging
}
```

**`data_utils/augment.py`**
Helper functions for random flips, scaling, and mosaic augmentation (combining 4 images into one). Called inside `dataset.py` when `augment=True`. Applied to **LR images and labels only** â€” never to HR images, which must stay clean as reconstruction targets. Output: modified tensors â€” not saved to disk.

---

### ðŸ“ `models/` â€” The Neural Network Architecture

**`models/backbone.py`**
Wraps pretrained YOLOv8n and attaches PyTorch hooks at layers 4 and 8 to intercept feature maps mid-flow â€” like tapping a pipe at two points to sample what is flowing. The main detection flow continues unaffected.

Output per forward pass:
```python
(yolo_predictions,  {4: Tensor[B, 64, 80, 80],   # fine spatial detail â†’ SR encoder
                     8: Tensor[B, 256, 20, 20]})  # semantic features  â†’ SR encoder
```

**`models/sr_branch.py`**
The super-resolution decoder. Takes the two feature maps from the backbone and upscales them 4Ã— into a full 1280Ã—1280 image. Built from `ResBlock` (no BatchNorm, following EDSR) and `UpsamplePS` (pixel shuffle upsampling). **Only called during training â€” never at inference.**

Output:
```python
Tensor [B, 3, 1280, 1280]   # reconstructed HR image
                              # compared against data/processed/hr/ images
```

**`models/sre_yolo.py`**
The combined model â€” wires `BackboneWithHooks` and `SRBranch` together. The `inference` flag is the core innovation of the paper:

```python
# Training â€” both branches active
model(image, inference=False)
# â†’ {"predictions": ..., "sr_output": Tensor[B, 3, 1280, 1280]}

# Inference â€” SR branch completely skipped
model(image, inference=True)
# â†’ predictions only   (speed identical to vanilla YOLOv8n)
```

---

### ðŸ“ `training/` â€” The Engine That Trains the Model

**`training/loss.py`**
Combines four loss terms into one number the optimiser minimises:

```
total_loss = 0.1Â·L_SR + 7.5Â·L_bbox + 1.5Â·L_dfl + 0.5Â·L_cls
```

Called once per batch. Its scalar output drives `loss.backward()` which computes gradients.

Output:
```python
{
  "total_loss": scalar tensor,  # the number that gets minimised
  "l_sr":       float,          # SR reconstruction quality
  "l_bbox":     float,          # bounding box accuracy
  "l_dfl":      float,          # box boundary distribution
  "l_cls":      float           # lesion classification
}
```

**`training/trainer.py`**
Orchestrates the full training loop:

```
for epoch in range(100):
    for batch in dataloader:          â† loads images from dataset.py
        predictions = model(batch)    â† calls sre_yolo.py
        loss = criterion(predictions) â† calls loss.py
        loss.backward()               â† computes gradients
        optimiser.step()              â† updates all weights
    validate()                        â† checks val AP after each epoch
    early_stop_check()                â† stops if no improvement for 50 epochs
    save_checkpoint()                 â† saves best model weights
```

Output:
```
weights/best_split1.pt    â† best model weights (main deliverable)
weights/last_split1.pt    â† most recent checkpoint for resuming
W&B dashboard             â† live loss curves and AP metrics
```

**`training/ablation.py`**
Runs a shortened 20-epoch training with one config override, then evaluates. Used only in Section 8 to systematically test every design choice in the paper.

Output:
```
results/ablations/sr_layers_48.json   â† {name, ap50, fps, gflops}
results/ablations/pretrain_coco.json
... (13 runs total)
results/ablation_summary.csv
```

---

### ðŸ“ `evaluation/` â€” Measures How Good the Trained Model Is

**`evaluation/metrics.py`**
Four measurement functions â€” called after training is complete using saved checkpoint weights:

| Function | What it measures | Target value |
|---|---|---|
| `compute_ap50()` | AP@IoU=0.5 on full test set | 0.82 |
| `compute_ap50_by_size()` | AP for small / medium / large lesions separately | small: 0.80 |
| `measure_fps()` | Frames per second â€” times 500 forward passes | 58.8 FPS |
| `measure_gflops()` | Arithmetic operations per forward pass | 8.2 GFLOPs |

Output:
```python
{
  "ap50": 0.82,
  "ap50_small": 0.80, "ap50_medium": 0.82, "ap50_large": 0.85,
  "fps": 58.8,
  "gflops": 8.2
}
```

---

### ðŸ“„ Root-Level Files â€” Entry Points

**`train.py`**
CLI entry point for training. Reads flags, loads config, builds model and dataset, hands everything to `Trainer.fit()`.
```bash
python train.py --split 1 --weights coco.pt
```

**`evaluate.py`**
Loads a saved checkpoint and runs `metrics.py` on the test set.
```bash
python evaluate.py --weights weights/best_split1.pt --split 1
```
Output: printed metrics table + optional CSV file.

**`infer.py`**
Runs the model on an image, video file, or live webcam. Draws green bounding boxes on lesions with confidence scores and FPS counter overlay.
```bash
python infer.py --weights weights/best_split1.pt --source video.mp4
```
Output: annotated video/images saved to `results/inference/`

**`export.py`**
Converts trained PyTorch model to ONNX format for deployment on any hardware without needing PyTorch installed.
```bash
python export.py --weights weights/best_split1.pt
```
Output: `weights/sre_yolo_inference.onnx`

**`run_all_ablations.py`**
Runs all 13 ablation experiments automatically by calling `ablation.py` with different config overrides â€” one for each design choice tested in the paper.
Output: `results/ablation_summary.csv` â€” the data behind Tables 4â€“8 in the paper.

**`tests/`**
All pytest test files â€” one per section (`test_data.py`, `test_model.py`, `test_loss.py`, etc.). 51 tests total. These catch bugs before they waste hours of GPU training time. Run all with:
```bash
pytest tests/ -v
```

---

### How It All Flows Together

```
configs/train.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                              â†“
data/raw/ â†’ preprocess.py â†’ data/processed/ â†’ dataset.py â†’ trainer.py
                                                    â†‘              â†“
                                                loss.py       sre_yolo.py
                                                    â†‘              â†‘
                                             backbone.py + sr_branch.py
                                                              â†“
                                               weights/best_split1.pt
                                                              â†“
                                 evaluate.py â†’ metrics.py â†’ results/
                                 infer.py â†’ results/inference/
                                 export.py â†’ weights/sre_yolo_inference.onnx
```

code ran
```bash
mkdir -p /home/rl1231/sre_yolo/configs /home/rl1231/sre_yolo/data/raw /home/rl1231/sre_yolo/data/processed/lr /home/rl1231/sre_yolo/data/processed/hr /home/rl1231/sre_yolo/data/splits/split1/train /home/rl1231/sre_yolo/data/splits/split1/val /home/rl1231/sre_yolo/data/splits/split1/test /home/rl1231/sre_yolo/data/splits/split2/train /home/rl1231/sre_yolo/data/splits/split2/val /home/rl1231/sre_yolo/data/splits/split2/test /home/rl1231/sre_yolo/data/splits/split3/train /home/rl1231/sre_yolo/data/splits/split3/val /home/rl1231/sre_yolo/data/splits/split3/test /home/rl1231/sre_yolo/models /home/rl1231/sre_yolo/data_utils /home/rl1231/sre_yolo/training /home/rl1231/sre_yolo/evaluation /home/rl1231/sre_yolo/tests /home/rl1231/sre_yolo/weights /home/rl1231/sre_yolo/results
```

### YAML Configuration Files Â· ðŸŸ¢ Beginner

YAML is a human-readable format for storing configuration. Instead of hardcoding hyperparameters in your Python files, you put them in a `.yaml` file. Changing a learning rate then means editing one line in one file, not hunting through code.

**ðŸ”— In this project:** `configs/train.yaml` holds all 19 hyperparameters: learning rate, loss weights (c1=0.1, c2=7.5...), image sizes, SR layer indices, and more.

- â–¶ [YAML Tutorial â€” Learn YAML in 10 Minutes](https://www.youtube.com/watch?v=BEki_rsWu4E) â€” syntax and use cases for YAML
- â–¶ [Python yaml Library â€” Loading Configs](https://www.youtube.com/watch?v=YPrST4VKfUQ) â€” reading YAML files in Python scripts

---

### Git Version Control Â· ðŸŸ¢ Beginner

Git tracks changes to your code over time. You can snapshot your work (commit), experiment on a copy (branch), and go back if something breaks. In ML, it also tracks which code version produced which experiment results.

**ðŸ”— In this project:** The spec asks you to `git commit` after every section. This means you always have a working checkpoint to return to if a later section breaks something.

- â–¶ [Git and GitHub for Beginners (freeCodeCamp)](https://www.youtube.com/watch?v=RGOj5yH7evk) â€” 1-hour complete beginner guide
- â–¶ [Git for Machine Learning â€” Practical Guide](https://www.youtube.com/watch?v=VzAkMRXQoOM) â€” ML-specific git workflows

---

### argparse â€” CLI for Python Scripts Â· ðŸŸ¢ Beginner

argparse lets you pass arguments to a Python script from the command line. Instead of editing code to change which dataset to use, you write: `python train.py --split 2 --weights coco.pt`.

**ðŸ”— In this project:** `train.py` uses argparse so you can switch between splits, load pretrained weights, and toggle W&B logging all from the command line without touching code.

- â–¶ [Python argparse Tutorial](https://www.youtube.com/watch?v=cdblJqEUDNo) â€” building command-line ML scripts

---


## ðŸ—„ï¸ Section 3 â€” Data Pipeline

> In ML, "garbage in, garbage out" is literal. The data pipeline section covers how raw endoscopy images are cleaned, resized, split, and served to the model. This is often 60% of a real project's work.

---

### Image Representation & OpenCV Â· ðŸŸ¢ Beginner

A digital image is a 3D array of numbers: height Ã— width Ã— colour channels (RGB). OpenCV is the standard library for image loading, resizing, and manipulation in Python.

**ðŸ”— In this project:** Every raw endoscopy frame is resized to 640Ã—640 (model input) AND 1280Ã—1280 (SR branch target). OpenCV handles both.

- â–¶ [OpenCV Python Tutorial for Beginners (freeCodeCamp)](https://www.youtube.com/watch?v=oXlwWbU8l2o) â€” 4-hour complete OpenCV course
- â–¶ [Image Processing Fundamentals](https://www.youtube.com/watch?v=gcNaQqZaTkI) â€” pixels, channels, and colour spaces explained

---

### YOLO Bounding Box Format Â· ðŸŸ¢ Beginner

YOLO format stores boxes as: `class_id, cx, cy, w, h` â€” all normalised to [0,1] relative to image size. cx/cy is the box centre. This differs from Pascal VOC format (x_min, y_min, x_max, y_max).

**ðŸ”— In this project:** All 3,892 frames use YOLO-format `.txt` label files. When you resize images from original size to 640Ã—640, you must also scale the bounding box coordinates accordingly.

- â–¶ [YOLO Object Detection Explained](https://www.youtube.com/watch?v=9s_FpMpdYW8) â€” what YOLO format means and why it works
- â–¶ [Bounding Box Formats Explained](https://www.youtube.com/watch?v=VqMNODV7Pgk) â€” YOLO vs Pascal VOC vs COCO formats

---

### Image Quality Filtering (Laplacian Variance) Â· ðŸŸ¡ Intermediate

The Laplacian operator computes second-order image derivatives â€” it responds strongly to edges. The variance of the Laplacian measures sharpness. A blurry image has low variance; a sharp image has high variance.

**ðŸ”— In this project:** `filter_frames()` uses Laplacian variance < 100 to remove blurry endoscopy frames before training. Noisy labels from blurry frames would hurt model accuracy.

- â–¶ [Blur Detection with OpenCV](https://www.youtube.com/watch?v=ukNqK5Qz0tA) â€” implementing Laplacian variance in Python
- â–¶ [Image Quality Assessment Techniques](https://www.youtube.com/watch?v=k_fLKuBB6wM) â€” sharpness, brightness, and artefact detection

---

### Train / Validation / Test Split Â· ðŸŸ¢ Beginner

You train on one portion of data, tune hyperparameters on validation, and report final performance on a held-out test set. The test set must NEVER influence training decisions.

**ðŸ”— In this project:** `make_splits()` divides 3,892 frames into train/val/test. The split must be patient-level â€” all frames from one patient stay together â€” otherwise the model could memorise patient appearance and report fake high accuracy.

- â–¶ [Train Test Split â€” Explained Visually](https://www.youtube.com/watch?v=_vdMKioCXqQ) â€” why you need 3 splits and how to do it
- â–¶ [Data Leakage in Machine Learning](https://www.youtube.com/watch?v=jsdfsXAESQU) â€” the silent killer of ML project credibility

---

### PyTorch Dataset & DataLoader Â· ðŸŸ¡ Intermediate

`torch.utils.data.Dataset` is an abstract class you subclass to teach PyTorch how to load one sample. `DataLoader` wraps a Dataset and handles batching, shuffling, and parallel loading.

**ðŸ”— In this project:** `SREYOLODataset` returns a dict with `lr_image [3,640,640]`, `hr_image [3,1280,1280]`, and `labels` for every frame. The DataLoader then batches these for efficient GPU training.

- â–¶ [PyTorch Dataset and DataLoader Explained](https://www.youtube.com/watch?v=PXOzkkB5eH0) â€” build a custom Dataset from scratch
- â–¶ [Custom DataLoader Tutorial](https://www.youtube.com/watch?v=ZoZHd0Zm3RY) â€” handles images, labels, and augmentation

---

### Data Augmentation Â· ðŸŸ¡ Intermediate

Augmentation artificially expands your dataset by applying random transformations to existing images. This prevents overfitting â€” the model can't memorise the training set if it looks slightly different every epoch.

**ðŸ”— In this project:** `SREYOLODataset` applies random flips, scaling, and Mosaic (combining 4 images) to LR images during training. Augmentation is NEVER applied to HR images.

- â–¶ [Data Augmentation for Object Detection](https://www.youtube.com/watch?v=GBCzM8VY7k8) â€” flip, scale, mosaic â€” and why they work
- â–¶ [Albumentations Library Tutorial](https://www.youtube.com/watch?v=rAdLwKJBvPM) â€” fastest augmentation library for PyTorch
- â–¶ [Mosaic Augmentation (YOLOv4 technique)](https://www.youtube.com/watch?v=v_N2zGVR7lw) â€” how 4-image mosaic improves small object detection

---

## ðŸ§  Section 4 â€” Model Architecture

> This is the heart of the paper. Three interlocking pieces: the YOLOv8 detector, an EDSR-inspired super-resolution branch, and the mechanism that fuses them during training but removes the SR branch at inference.

---

### Convolutional Neural Networks (CNNs) Â· ðŸŸ¢ Beginner

CNNs apply small learnable filters (kernels) across an image, detecting features like edges, textures, and shapes. Deeper layers combine these into complex representations.

**ðŸ”— In this project:** Both the YOLOv8n backbone and the SR branch are built from Conv2d layers. Understanding `kernel_size`, `stride`, and `padding` is essential for reading the architecture spec.

- â–¶ [But what is a Convolutional Neural Network? (3Blue1Brown)](https://www.youtube.com/watch?v=KuXjwB4LzSA) â€” best visual intuition for CNNs â€” 20 min
- â–¶ [CNN Explained â€” Stanford CS231n](https://www.youtube.com/watch?v=bNb2fEVKeEo) â€” rigorous explanation of conv, pooling, strides
- â–¶ [nn.Conv2d Parameters Explained](https://www.youtube.com/watch?v=y2BaTt1fxJU) â€” kernel, stride, padding â€” practical PyTorch

---

### Object Detection & YOLO Â· ðŸŸ¡ Intermediate

Object detection predicts both WHAT is in an image and WHERE (bounding box). YOLO does this in a single forward pass. YOLOv8 uses a CSP backbone, PAN neck, and decoupled head.

**ðŸ”— In this project:** SRE-YOLO is built on YOLOv8n (nano). You hook into its backbone layers 4 and 8 to extract feature maps for the SR branch.

- â–¶ [YOLO Object Detection Explained (Computerphile)](https://www.youtube.com/watch?v=ag3DLKsl2vk) â€” conceptual walkthrough
- â–¶ [YOLOv8 Architecture Deep Dive](https://www.youtube.com/watch?v=wdN1_TL_u5Q) â€” backbone, neck, and head explained
- â–¶ [YOLOv8 with Ultralytics â€” Full Tutorial](https://www.youtube.com/watch?v=m9fH9OWn8YM) â€” practical usage of the Ultralytics library

---

### Feature Maps & Backbone Hooks Â· ðŸŸ¡ Intermediate

As an image passes through a CNN, each layer produces a feature map â€” a tensor encoding what the network has detected at that depth. PyTorch hooks intercept these intermediate tensors without modifying the model.

**ðŸ”— In this project:** Hooks are registered on layers 4 (64 channels, fine spatial detail) and 8 (256 channels, semantic features). The ablation study shows layers 4+8 outperform layers 2+6 by +2% AP.

- â–¶ [PyTorch Forward Hooks Tutorial](https://www.youtube.com/watch?v=UOvPeC8WOt8) â€” registering hooks to capture intermediate tensors
- â–¶ [Feature Visualization in Neural Networks](https://www.youtube.com/watch?v=ghEmQSxT6tw) â€” what feature maps actually look like

---

### Residual Networks & ResBlocks Â· ðŸŸ¡ Intermediate

A residual connection adds the input of a block directly to its output: `output = F(x) + x`. This solves the vanishing gradient problem and enables very deep networks to train effectively.

**ðŸ”— In this project:** The SR decoder uses 16 residual blocks. BatchNorm is removed from each block (following EDSR) to preserve the full pixel-value range for super-resolution quality.

- â–¶ [Residual Networks Explained (ResNet)](https://www.youtube.com/watch?v=GWt6Fu05voI) â€” skip connections and why they matter â€” 15 min
- â–¶ [ResNet Paper Walkthrough](https://www.youtube.com/watch?v=sAl7W4_kFoA) â€” deep dive into He et al. 2015 paper

---

### Super-Resolution & EDSR Â· ðŸ”µ Advanced

Super-resolution reconstructs a high-resolution image from a low-resolution input. EDSR (Lim et al. 2017) removed BatchNorm from residual blocks â€” preserving absolute pixel range information that SR critically depends on.

**ðŸ”— In this project:** The SR branch forces the shared backbone features to encode fine spatial detail during training, which benefits the detector for small lesions. At inference, the branch is removed completely.

- â–¶ [Super Resolution Deep Learning Explained](https://www.youtube.com/watch?v=KULkSwLk62I) â€” SR concepts, loss functions, and architectures
- â–¶ [EDSR Paper Explained](https://www.youtube.com/watch?v=yp7c5sMYEUI) â€” why removing BatchNorm improves SR
- â–¶ [Pixel Shuffle Upsampling Explained](https://www.youtube.com/watch?v=Vk-eWA-4pao) â€” sub-pixel convolution for fast upscaling

---

### Pixel Shuffle (Sub-pixel Convolution) Â· ðŸ”µ Advanced

Pixel shuffle rearranges a tensor `[B, CÃ—rÂ², H, W]` into `[B, C, HÃ—r, WÃ—r]` â€” upscaling spatial dimensions by factor r without checkerboard artefacts.

**ðŸ”— In this project:** Two `UpsamplePS` blocks (each with r=2) achieve a total 4Ã— spatial upscale: from `[B, C, 320, 320]` â†’ `[B, 3, 1280, 1280]`.

- â–¶ [Pixel Shuffle / Sub-pixel Convolution Explained](https://www.youtube.com/watch?v=Vk-eWA-4pao) â€” visual walkthrough of how pixel shuffle upscales
- â–¶ [nn.PixelShuffle in PyTorch](https://www.youtube.com/watch?v=5V6sSKMSAko) â€” implementing pixel shuffle from scratch

---

## ðŸ“‰ Section 5 â€” Loss Function

> The loss function is the signal that drives learning. SRE-YOLO uses a carefully weighted combination of four loss terms.

---

### Loss Functions & Gradient Descent Â· ðŸŸ¢ Beginner

A loss function measures how wrong the model's predictions are. Gradient descent uses backpropagation to find the direction that reduces the loss, then updates weights by a small step.

**ðŸ”— In this project:** The combined SRE-YOLO loss has four terms. Gradients flow back through both the detection head AND the SR branch, so both benefit from every training step.

- â–¶ [Loss Functions Explained](https://www.youtube.com/watch?v=Skc8nqJirJg) â€” MSE, L1, cross-entropy â€” when to use which
- â–¶ [Gradient Descent, Step by Step (StatQuest)](https://www.youtube.com/watch?v=sDv4f4s2SB8) â€” clear visual explanation â€” highly recommended
- â–¶ [Backpropagation Explained (3Blue1Brown)](https://www.youtube.com/watch?v=Ilg3gGewQ5U) â€” the chain rule made visual â€” essential viewing

---

### L1 Loss (Mean Absolute Error) Â· ðŸŸ¢ Beginner

L1 loss sums absolute differences: `L = mean(|y_pred - y_true|)`. More robust to outliers than L2. In image reconstruction, L1 often produces sharper results than L2.

**ðŸ”— In this project:** `L_SR = L1(SR_output, HR_ground_truth)`. Minimising this forces the network to reconstruct fine details accurately.

- â–¶ [L1 vs L2 Loss â€” When to Use Each](https://www.youtube.com/watch?v=65o6GDFUMvM) â€” MAE vs MSE tradeoffs for regression and SR

---

### IoU Loss & Bounding Box Regression Â· ðŸŸ¡ Intermediate

IoU (Intersection over Union) measures overlap between predicted and ground-truth boxes. A perfect prediction has IoU=1. YOLOv8 uses CIoU which also penalises centre distance and aspect ratio mismatch.

**ðŸ”— In this project:** `L_bbox` has coefficient c2=7.5 â€” the highest of the four, reflecting that accurate box localisation is the primary detection goal.

- â–¶ [IoU Explained for Object Detection](https://www.youtube.com/watch?v=XXYG5ZWtjj0) â€” IoU, GIoU, DIoU, CIoU â€” visual walkthrough
- â–¶ [Bounding Box Regression Loss Functions](https://www.youtube.com/watch?v=7LV0tBl0bYw) â€” how YOLO boxes are trained

---

### Distribution Focal Loss (DFL) Â· ðŸ”µ Advanced

Instead of predicting a single coordinate value, the model predicts a probability distribution over possible values. DFL is the cross-entropy loss between this distribution and a one-hot target. It leads to more precise box edges.

**ðŸ”— In this project:** `L_dfl` (c3=1.5) is specific to YOLOv8's decoupled detection head, which predicts box boundaries as distributions.

- â–¶ [DFL â€” Distribution Focal Loss Explained](https://www.youtube.com/watch?v=l9OKOW7Kkng) â€” how YOLOv8 predicts box edges as distributions

---

### Weighted Multi-Task Loss Â· ðŸŸ¡ Intermediate

When training on multiple objectives simultaneously, you combine their losses with scalar weights. The weights control how much each objective influences shared feature representations.

**ðŸ”— In this project:** `total = 0.1Â·L_SR + 7.5Â·L_bbox + 1.5Â·L_dfl + 0.5Â·L_cls`. The small SR weight (0.1) prevents the SR branch from dominating training and harming detection accuracy.

- â–¶ [Multi-Task Learning Loss Weighting](https://www.youtube.com/watch?v=qdRqjJiQHhg) â€” how to balance competing loss terms

---

## ðŸ‹ï¸ Section 6 â€” Training Pipeline

> The training pipeline orchestrates everything: optimiser, learning rate schedule, early stopping, checkpointing, and experiment logging.

---

### Optimisers: AdamW Â· ðŸŸ¡ Intermediate

AdamW maintains a per-parameter learning rate based on gradient moments â€” much faster than SGD. The W suffix adds weight decay correctly, separate from the gradient update.

**ðŸ”— In this project:** Trainer uses AdamW with `lr0=0.05` and `weight_decay=1e-4`.

- â–¶ [Adam Optimizer Explained â€” StatQuest](https://www.youtube.com/watch?v=JXQT_vxqwIs) â€” intuitive explanation of adaptive learning rates
- â–¶ [AdamW vs Adam â€” What's the Difference?](https://www.youtube.com/watch?v=0WRTelebsS4) â€” why weight decay matters

---

### Learning Rate Scheduling Â· ðŸŸ¡ Intermediate

Cosine Annealing smoothly decreases the LR following a cosine curve from `lr_max` to `lr_min`, allowing fine-grained convergence.

**ðŸ”— In this project:** `CosineAnnealingLR(T_max=100, eta_min=0.01)` decreases LR from 0.05 to 0.01 over 100 epochs.

- â–¶ [Learning Rate Schedules Explained](https://www.youtube.com/watch?v=QzulmoOg2JE) â€” step, cosine, warmup â€” visual comparison
- â–¶ [Cosine Annealing in PyTorch](https://www.youtube.com/watch?v=SKYMzNm7UoM) â€” implementing and visualising cosine scheduling

---

### Transfer Learning & Fine-tuning Â· ðŸŸ¡ Intermediate

Starting from a model pretrained on a large dataset (COCO: 118K images) and fine-tuning on the target dataset dramatically reduces the data needed for good results.

**ðŸ”— In this project:** Without COCO pretraining, the paper shows AP drops from 0.77 to 0.71 (Table 4) â€” a 6-point penalty for skipping this step.

- â–¶ [Transfer Learning Explained Visually](https://www.youtube.com/watch?v=yofjFQddwHE) â€” why pretrained features transfer to new tasks
- â–¶ [Fine-tuning YOLO on Custom Data](https://www.youtube.com/watch?v=0inNp1M8OBw) â€” practical COCO â†’ custom fine-tune walkthrough

---

### Overfitting & Early Stopping Â· ðŸŸ¢ Beginner

Overfitting is when a model memorises training data instead of learning generalisable patterns. Early stopping halts training when validation performance stops improving.

**ðŸ”— In this project:** `patience=50` means training stops if validation AP does not improve for 50 consecutive epochs. With only 3,452 frames, overfitting is a real risk.

- â–¶ [Overfitting and Underfitting Explained](https://www.youtube.com/watch?v=EuBBz3bI-aA) â€” the bias-variance tradeoff with visuals
- â–¶ [Early Stopping in Neural Networks](https://www.youtube.com/watch?v=NnS0FJyVcDQ) â€” implementing and tuning patience

---

### Experiment Tracking with Weights & Biases Â· ðŸŸ¡ Intermediate

W&B logs metrics, hyperparameters, and model outputs to an online dashboard. You can compare multiple runs side-by-side and reproduce any experiment.

**ðŸ”— In this project:** `wandb.log()` is called every batch (loss) and every epoch (AP). The dashboard lets you watch the SR loss and detection AP co-evolve during training.

- â–¶ [Weights & Biases (W&B) Full Tutorial](https://www.youtube.com/watch?v=G7GH0SeNBMA) â€” setup, logging, and comparing runs

---

## ðŸ“Š Section 7 â€” Evaluation & Metrics

---

### Mean Average Precision (AP@IoU=0.5) Â· ðŸŸ¡ Intermediate

AP computes the area under the Precision-Recall curve at a given IoU threshold. AP@0.5 means a detection counts as correct if its IoU with the ground-truth box is â‰¥ 0.5.

**ðŸ”— In this project:** The paper's main result is SRE-YOLO achieves AP@0.5 = 0.82 vs baseline 0.77 (+5%).

- â–¶ [Mean Average Precision (mAP) Explained](https://www.youtube.com/watch?v=FppOzcDvaDI) â€” the most thorough AP explanation on YouTube
- â–¶ [Precision, Recall, and F1 Score](https://www.youtube.com/watch?v=jJ7ff7Gcubg) â€” foundations needed before understanding mAP

---

### GFLOPs â€” Computational Complexity Â· ðŸŸ¡ Intermediate

GFLOPs measures arithmetic operations per forward pass. Hardware-independent, unlike FPS.

**ðŸ”— In this project:** SRE-YOLO has identical GFLOPs (8.2) to baseline at inference because the SR branch is completely removed. +5% AP at zero inference cost.

- â–¶ [FLOPs and Model Complexity Explained](https://www.youtube.com/watch?v=RxOFNJQ2WLs) â€” GFLOPs, parameters, and inference speed

---

### Small Object Detection Â· ðŸ”µ Advanced

Small objects (< 32Ã—32 pixels) are notoriously hard to detect â€” they occupy few pixels and are disproportionately affected by feature map downsampling in deep networks.

**ðŸ”— In this project:** SRE-YOLO improves AP for small lesions from 0.66 â†’ 0.80 (+21%) on ENDO-LC ext. This is the most clinically significant result in the paper.

- â–¶ [Small Object Detection â€” Challenges and Solutions](https://www.youtube.com/watch?v=r9IQDQkSv_w) â€” why small objects are hard and how SR helps

---

## ðŸ”¬ Section 8 â€” Ablation Studies

---

### Ablation Studies â€” What and Why Â· ðŸŸ¢ Beginner

Ablation studies isolate each variable: change only X, keep everything else the same, measure the difference. This is the scientific method applied to neural network design.

**ðŸ”— In this project:** 5 ablation sets test pre-training dataset, SR layer placement, SR architecture, and backbone size â€” justifying every design choice in SRE-YOLO.

- â–¶ [How to Read ML Research Papers](https://www.youtube.com/watch?v=733m6qBH-jI) â€” understanding tables, ablations, and results sections
- â–¶ [Ablation Studies in Deep Learning Research](https://www.youtube.com/watch?v=K0_GdBz3Y00) â€” why ablations are the core of experimental ML papers

---

## ðŸš€ Section 9 â€” Inference & Deployment

---

### Inference vs Training Mode Â· ðŸŸ¢ Beginner

During inference, `model.eval()` and `torch.no_grad()` disable gradient tracking, saving 2â€“3Ã— memory and compute. Dropout and BatchNorm also behave differently in eval mode.

**ðŸ”— In this project:** `SREYOLO.forward(x, inference=True)` completely skips the SR branch â€” the core innovation. The SR branch is a training-time regulariser, not an inference-time cost.

- â–¶ [PyTorch model.eval() vs model.train()](https://www.youtube.com/watch?v=GtPnWjnC90A) â€” dropout, batchnorm, and gradient modes explained

---

### ONNX â€” Model Export & Portability Â· ðŸŸ¡ Intermediate

ONNX is a universal format for representing neural networks, decoupling your model from PyTorch. It can run on ONNX Runtime, TensorRT, CoreML, OpenVINO â€” any hardware.

**ðŸ”— In this project:** `export.py` converts inference-mode SRE-YOLO (SR branch removed) to ONNX. Tests verify outputs match PyTorch to within 1e-4.

- â–¶ [ONNX Export with PyTorch â€” Full Tutorial](https://www.youtube.com/watch?v=7nutT3Aacyw) â€” torch.onnx.export step by step
- â–¶ [ONNX Runtime for Fast Inference](https://www.youtube.com/watch?v=UUL5rHBHUA4) â€” running ONNX models in production

---

### Non-Maximum Suppression (NMS) Â· ðŸŸ¡ Intermediate

NMS removes duplicate overlapping bounding boxes: keep the highest-confidence box, suppress all boxes that overlap it by more than an IoU threshold.

**ðŸ”— In this project:** `infer.py` applies NMS with `conf_thresh=0.25` and `iou_thresh=0.45`.

- â–¶ [Non-Maximum Suppression Explained](https://www.youtube.com/watch?v=VAo84c1hQX8) â€” visual walkthrough of how NMS works

---

## ðŸ§ª Section 10 â€” Integration Testing

---

### pytest â€” Python Testing Framework Â· ðŸŸ¢ Beginner

pytest is the standard Python testing library. Functions starting with `test_` are automatically discovered and run. Fixtures create reusable test data.

**ðŸ”— In this project:** Every section ends with pytest commands. 51 test cases across 10 sections verify the full pipeline end-to-end.

- â–¶ [pytest Tutorial for Beginners (Corey Schafer)](https://www.youtube.com/watch?v=cHYq1MRoyI0) â€” setup, fixtures, parametrize â€” complete guide

---

### Synthetic Data for Testing Â· ðŸŸ¡ Intermediate

Tests using synthetic data (random noise images with random labels) run in milliseconds, need no external files, and test the exact same code paths as real data.

**ðŸ”— In this project:** All `tests/test_*.py` files create synthetic 640Ã—640 images in a temp directory. pytest runs completely self-contained.

- â–¶ [Generating Synthetic Test Data in Python](https://www.youtube.com/watch?v=VlFxoFZs_-Q) â€” numpy-based synthetic image and label generation

---

## ðŸ“š Recommended Learning Path

| When | What to Watch / Read | Covers |
|---|---|---|
| Before Â§1â€“2 | 3Blue1Brown: Neural Networks series (4 videos) | Intuition for how neural nets learn |
| Before Â§3 | PyTorch Beginner Series (official docs) | Tensors, autograd, nn.Module |
| Before Â§3 | OpenCV Python Tutorial (freeCodeCamp, 4hr) | Image loading, resizing, drawing |
| Before Â§4 | Stanford CS231n Lecture 5 â€” CNNs | Convolution, pooling, strides |
| Before Â§4 | YOLO Object Detection Explained | YOLO architecture and format |
| Before Â§5 | StatQuest: Gradient Descent | Backprop and loss functions |
| Before Â§6 | W&B Tutorial â€” Experiment Tracking | W&B setup and logging |
| Before Â§7 | Mean Average Precision Explained | mAP, AP@0.5, PR curves |
| Before Â§9 | ONNX Export with PyTorch | Model export and portability |
| Before Â§10 | pytest Tutorial (Corey Schafer) | Writing and running tests |

> ðŸ’¡ **Tip:** You will learn far more by building and breaking things than by watching videos. Start coding early, use the YouTube resources when you get stuck, and trust the tests to tell you when something is right.
